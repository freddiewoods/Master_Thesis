{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicated(df):\n",
    "    \"\"\"\n",
    "    Removes duplicated entries and nan values from the dataset.\n",
    "     \n",
    "    Input: \n",
    "        df: dataframe; the entire dataset\n",
    "    Output:\n",
    "        df: dataframe; the dataframe without duplicates.    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    df = df.dropna(how = 'any')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_demographics(path):\n",
    "    \"\"\"\n",
    "    Loads in user demographic and refines the stats to contain the required stats (uuid, age, country)\n",
    "    \n",
    "    Input\n",
    "        path: string; the file path for the user_demographic.csv file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, delimiter = '\\t')\n",
    "    # Extracts required stats.\n",
    "    refined_df = df[['uuid','age','country']]    \n",
    "    refined_df = refined_df.set_index('uuid')    \n",
    "    return(refined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_files(path,refined_demographics):\n",
    "    \"\"\"\n",
    "    Combines the demographics files with the newly loaded users file with the redfined_demographic files\n",
    "    \n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.csv'):\n",
    "            # Loads each of the users in the users.pkl\n",
    "            df = pd.read_csv(path + \"/\" + file, index_col = [0])\n",
    "            df = drop_duplicated(df)\n",
    "            # Uses an left in join to make sure only required demographics are loaded.\n",
    "            df = pd.concat([df,refined_demographics], axis = 1, join = 'inner')\n",
    "            files.append(df)\n",
    "\n",
    "    users = pd.concat(files)\n",
    "    users.to_pickle(\"../../cleaned_data/users.pkl\") \n",
    "    return(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_users():\n",
    "    \"\"\"\n",
    "    Loads each of the users, if the users pickle file is not found then it will load users from the origianl dataset.\n",
    "    \"\"\"\n",
    "    if \"users.pkl\" in os.listdir(\"../cleaned_data\"):\n",
    "        return(pd.read_pickle(\"../cleaned_data/users.pkl\"))\n",
    "    else:\n",
    "        print(\"Cleaned File not found ....\")\n",
    "        print(\"Generating File\")\n",
    "        refined = refine_demographics(\"..\")\n",
    "        refined = drop_duplicated(refined)\n",
    "        return(join_files(\"../cleaned_data/users.pkl\", refined))\n",
    "#users = load_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_artist_name(artist_lookup, artists):\n",
    "    \"\"\"\n",
    "    Adds the artist name to each of the tracks based off the MBID.\n",
    "    \n",
    "    Input \n",
    "        artist_lookup: dictionary {artistMBID:artist_name} the lookup dictionary to obtain an artist's name\n",
    "        artists: the dataframe of artists.\n",
    "    Return\n",
    "        merged: Dataframe, the merged file.\n",
    "    \"\"\"\n",
    "    merged = pd.merge(artist_lookup, artists, how='left', on = 'MBID')\n",
    "    return(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_artists():\n",
    "    \"\"\"\n",
    "    Reads the aritists dataframe, if the artists.pkl file is not found then the data is loaded using the mbdump.\n",
    "    \n",
    "    Output\n",
    "        artists: Dataframe, the stored artists metadata.\n",
    "    \"\"\"\n",
    "    if \"artists.pkl\" in os.listdir('../cleaned_data'):\n",
    "        print(\"Cleaned data found\")\n",
    "        artists = pd.read_pickle('../cleaned_data/artists.pkl', index_col = 0)\n",
    "        return(artists)\n",
    "        \n",
    "    else:\n",
    "        print(\"Cleaned data not found: Loading file.....\")\n",
    "        print(\"Loading File....\")\n",
    "        artists = pd.read_csv('../dirty_data/mbdump/mbdump/artist', '\\t', header = None)\n",
    "        artists.columns = ['pkID', 'MBID', 'Artist', '3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n",
    "        artists = artists[['pkID','MBID', 'Artist']]\n",
    "        artists.to_pickle('../cleaned_data/artists.pkl')\n",
    "        return(artists)\n",
    "    \n",
    "#artists = read_artists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_recording():\n",
    "    \"\"\"\n",
    "    Reads the recordings file. If the recordings.pkl file is not found then it is generated using the mbdump.\n",
    "    \n",
    "    Output:\n",
    "        recordings: pandas Dataframe, the recordings metadata.\n",
    "    \n",
    "    \"\"\"\n",
    "    if \"recordings.pkl\" in os.listdir('../cleaned_data'):\n",
    "        print(\"Cleaned data found\")\n",
    "        recordings = pd.read_pkl('../cleaned_data/recordings.pkl', index_col = 0)\n",
    "        return(recordings)\n",
    "        \n",
    "    else:\n",
    "        print(\"Cleaned data not found: Loading file.....\")\n",
    "        print(\"Loading File....\")\n",
    "        recordings = pd.read_csv('../dirty_data/mbdump/mbdump/recording', '\\t', header = None)\n",
    "        recordings.columns = ['pkID', 'MBID', 'Artist', '3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n",
    "        # Reduce the dataset to just required files.\n",
    "        recordings = recordings[['pkID','MBID', 'Recording']]\n",
    "        recordings.to_pickle('../cleaned_data/recordings.pkl')\n",
    "        return(recor)\n",
    "    \n",
    "#artists = read_artists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_convert(df):\n",
    "    \"\"\"\n",
    "    Replaces the string based uuid used for identifying users to an interger based ID\n",
    "    \n",
    "    Inputs\n",
    "        df: Dataframe, the sample dataset used within this file\n",
    "    \n",
    "    Output\n",
    "        df: Dataframe, the modified dataset with userID added.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Obtains the unique values.\n",
    "    item_lookup = pd.Series(dict(enumerate(df.item.unique())), name = 'MBID')\n",
    "    user_lookup = pd.Series(dict(enumerate(df.userID.unique())))\n",
    "    \n",
    "    # Creates a lookup pandas series\n",
    "    user_lookup_2 = (pd.Series(user_lookup.index.values, index=user_lookup ))\n",
    "    lookup_2 = (pd.Series(item_lookup.index.values, index=item_lookup))\n",
    "    \n",
    "    # Renames the column to be more understandble  \n",
    "    df = df.rename(columns = {'userID':'userMBID', item:(item-'ID')+('MBID')})\n",
    "    \n",
    "    # Maps the lookup values to the correlating columns.\n",
    "    df['userID'] = df['userMBID'].map(user_lookup_2)\n",
    "    df[item] = df[(item-'ID')+('MBID')].map(item_lookup_2)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lookups_tracks(df,tracks):\n",
    "    \"\"\"\n",
    "    Maps an integer based trackID to each recording-MBID, further more the track name is added to the dataset\n",
    "    \n",
    "    Inputs\n",
    "        df: Dataframe, the sample dataset used within this file\n",
    "    \n",
    "    Output\n",
    "        df: Dataframe, the modified dataset with trackID and track name added.\n",
    "    \"\"\"\n",
    "    \n",
    "    item_lookup = pd.Series(dict(enumerate(df[\"recording-MBID\"].unique())))\n",
    "    user_lookup = pd.Series(dict(enumerate(df.uuid.unique())))\n",
    "    \n",
    "    user_lookup_2 = (pd.Series(user_lookup.index.values, index=user_lookup ))\n",
    "    lookup_2 = (pd.Series(item_lookup.index.values, index=item_lookup))\n",
    "     \n",
    "    df = df.rename(columns = {'uuid':'userMBID', 'recording-MBID':'recordingMBID'})\n",
    "    \n",
    "    df['userID'] = df['userMBID'].map(user_lookup_2)\n",
    "    df['songID'] = df['recordingMBID'].map(lookup_2)\n",
    "\n",
    "    # Adds track name to the dataset.\n",
    "    df['trackName'] = pd.merge(df,tracks[['recordingMBID','trackName']], on = 'recordingMBID', how = 'inner')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
