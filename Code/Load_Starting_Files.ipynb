{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pyspark as ps\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads Each of the required files for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/18 23:59:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/18 23:59:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Creates a pyspark session to load the data with,\n",
    "spark = SparkSession.builder.master(\"local\").config(conf=SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the files using pyspark (for speed and efficiency), extracting the relevant data from each record per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads one file from the MLHD dataset.\n",
    "    \n",
    "    Input:\n",
    "        file_path; string\n",
    "    Return:\n",
    "        user_without_null; spark dataframe, the file where null values have been droped.\n",
    "    \"\"\"\n",
    "    user = spark.read.format(\"text\").load(file_path)\n",
    "\n",
    "    # Loads files with column names\n",
    "    user_with_col =user.withColumn(\"timestamp\",f.split(user.value,\"\\s+\")[0].cast(\"int\")).\\\n",
    "                                     withColumn(\"artist-MBID\",f.split(user.value,\"\\s+\")[1].cast(\"string\")).\\\n",
    "                                     withColumn(\"release-MBID\",f.split(user.value,\"\\s+\")[2].cast(\"string\")).\\\n",
    "                                     withColumn(\"recording-MBID\",f.split(user.value,\"\\s+\")[3].cast(\"string\")) \n",
    "    \n",
    "    # Drops rows with empty values.\n",
    "    user_without_null = user_with_col.drop()\n",
    "    \n",
    "    # Checks if the file is empty\n",
    "    if user_without_null.count() == 0:\n",
    "        print(\"NONE\")\n",
    "        return(None)\n",
    "    else:\n",
    "        return(user_without_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_listens(df):\n",
    "    return(df.count())\n",
    "\n",
    "def get_unique_songs(df):\n",
    "    return(df.select(df[\"recording-MBID\"]).distinct().count())\n",
    "\n",
    "def get_max_repeated(df):\n",
    "    return(df.groupBy(df[\"recording-MBID\"])\\\n",
    "        .count()\\\n",
    "        .where(f.col('count') > 1)\\\n",
    "        .select(f.max('count'))\\\n",
    "        .first()[0])\n",
    "    \n",
    "    \n",
    "\n",
    "def get_exploratoryness(df):\n",
    "    L = df.count()\n",
    "    distinct = df.groupBy(\"recording-MBID\").count().toPandas()\n",
    "    return(1-((1/L)*np.sum((1/distinct[\"count\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(df):\n",
    "    \"\"\"\n",
    "    Extracts the basic stats for each user or the total dataset.\n",
    "    \n",
    "    Input \n",
    "        df: Dataframe, the test dataset\n",
    "    Output\n",
    "        dictionary {string, value}, the stats obtained from the dataset.\n",
    "    \"\"\"\n",
    "    total_listens = total_listens(df)\n",
    "\n",
    "    unique_songs = get_unique_songs(df)\n",
    "\n",
    "    repeated_songs = get_max_repeated(df)\n",
    "\n",
    "    exploratoryness = get_exploratoryness(df)\n",
    "\n",
    "    return({\"total_listens\": total_listens, \"unique_songs\": unique_songs, \"repeated_songs\": repeated_songs, \"exploratoryness\" : exploratoryness})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song Count Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_count(df):\n",
    "    \"\"\"\n",
    "    Calculates how many times a user listens a track\n",
    "    \n",
    "    Input:\n",
    "        df; Dataframe, the user dataset\n",
    "    Output\n",
    "        grouped df, the number of times a track is listened to by the user.\n",
    "    \"\"\"\n",
    "    return(df.groupBy('recording-MBID').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_features (directory):\n",
    "    \"\"\"\n",
    "    Loads in users with feature extraction for EDA.\n",
    "    \n",
    "    Input\n",
    "        directory; string, the file path for the directory of users.\n",
    "    Output\n",
    "        df; Dataframe, dataframe consisting of each users' features  \n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns = [\"n_listens\", \"n_unique\", \"n_same\", \"exploratoryness\", \"mainsteamness\"])\n",
    "    i = 0\n",
    "    # loop through each external file\n",
    "    for ex_directory in os.listdir(directory):\n",
    "        if \".tar\" in ex_directory or \".DS_Store\" in ex_directory:\n",
    "            print(ex_directory)\n",
    "        else:\n",
    "            i+=1\n",
    "            # Key stats recorded.\n",
    "            inter_df = pd.DataFrame(columns = [\"n_listens\", \"n_unique\", \"n_same\", \"exploratoryness\"])\n",
    "            print(\"Processing file: \" + ex_directory + \"    file number: \" + str(i))\n",
    "            #Loop through each internal file\n",
    "            print(\"Internal Files: \", len(os.listdir(directory + \"/\" + ex_directory)))\n",
    "            for file in os.listdir(directory + \"/\" + ex_directory):\n",
    "                if file.endswith(\".txt.gz\"):\n",
    "                    file_path = directory + \"/\" + ex_directory +\"/\" +file\n",
    "                    uuid = file.strip(\".txt.gz\")\n",
    "                    # Files loaded using spark to reduce time cost.\n",
    "                    spark_file = load_file(file_path)\n",
    "                    if spark_file != None:\n",
    "                        # Extracts desired information about each user.\n",
    "                        entry = extract_information(spark_file)\n",
    "                        inter_df.loc[uuid] = entry\n",
    "            \n",
    "            df = pd.concat([df,inter_df])\n",
    "\n",
    "    # Saves the dataframe as user_features\n",
    "    df.to_csv(\"../../user_features.csv\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_count (users, file_number):\n",
    "    \"\"\"\n",
    "    Loads files, while calculating the number of times a track is listened to by a user. \n",
    "    This is the desired file structure for the recommendation system\n",
    "    \n",
    "    Input\n",
    "        users; pandas series, a list of whitelisted users.\n",
    "        file_number; int, the file buffer refering to the next file to load in.\n",
    "    Output\n",
    "        value_count; Dataframe, the grouped data structure used for the RS model\n",
    "        bool; whether the final file has been downloaded\n",
    "        file_number; int, the current number refering to the current file downloaded\n",
    "    \"\"\"\n",
    "    \n",
    "    directory = '../dirty_data/MLHD-SS-00'\n",
    "\n",
    "    count = pd.DataFrame(columns = [\"uuid\", \"recording-MBID\", \"count\", \"artistName\"])\n",
    "    \n",
    "    files = []\n",
    "        \n",
    "    # Loop through each file directory based on the file buffer (filenumber)\n",
    "    for ex_directory in os.listdir(directory)[file_number:]:\n",
    "        file_number += 1\n",
    "        if \".tar\" in ex_directory or \".DS_Store\" in ex_directory:\n",
    "            print(ex_directory)\n",
    "        else:            \n",
    "             for file in (os.listdir(directory+\"/\"+ex_directory)):\n",
    "                if file.endswith(\".txt.gz\"):\n",
    "                    file_path = directory + \"/\"+ex_directory+ \"/\" +file\n",
    "                    uuid = file.strip(\".txt.gz\")\n",
    "                    if uuid in users.index:\n",
    "                        # Load data using spark.\n",
    "                        spark_file = load_file(file_path)\n",
    "                        if spark_file != None:                     \n",
    "                            # Group by the the recording-MBID to calculate number of times a track is listened to,\n",
    "                            df = (spark_file.groupBy('recording-MBID','artist-MBID').count().toPandas())\n",
    "                            df = df[df['recording-MBID'] != \"\"]\n",
    "                            # Remove null values.\n",
    "                            df = df.dropna()\n",
    "                            # Assign the uuid to the users listening history\n",
    "                            df['uuid'] = uuid\n",
    "                            \n",
    "                            files.append(df)\n",
    "        # Record in batches of 300 to prevent overloading memory\n",
    "        if len(files) > 300:\n",
    "            count = pd.concat(files)\n",
    "            # if it is not the final file then repeat with new file number\n",
    "            return(count, False, file_number)\n",
    "    value_count = pd.concat(files)\n",
    "    # If it is the final file then the loading has finished\n",
    "    return(value_count, True, file_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_track_names(data):\n",
    "    \"\"\"\n",
    "    Adds the track names to the database using the recording mbdump\n",
    "    \n",
    "    Input\n",
    "        data; Dataframe, the dataset implemented in the simulation\n",
    "    Output\n",
    "        filtered_data; Dataframe, the dataset with track names added.\n",
    "    \"\"\"\n",
    "    \n",
    "    # tracks loaded in using spark since its so large\n",
    "    tracks = spark.read.csv(\"../../dirty_data/mbdump/mbdump/recording\", sep = '\\t')\n",
    "    tracks = tracks.withColumnRenamed(\"_c1\",\"recordingMBID\")\\\n",
    "                   .withColumnRenamed(\"_c2\",\"trackName\")\n",
    "    tracks = tracks.drop(*['_c0','_c3','_c4','_c5','_c6','_c7','_c8'])\n",
    "    \n",
    "    # the data is temporalily stored since its quicker to merge two spark files\n",
    "    data.to_csv('../../temp/data.csv')\n",
    "    data_spark = spark.read.option(\"header\", \"true\").csv(\"../../temp/data.csv\")\n",
    "    data_spark = data_spark.drop(*[\"_c0\"])\n",
    "    \n",
    "    filtered_spark = data_spark.join(tracks, data_spark['recording-MBID'] == tracks[\"recordingMBID\"], 'left')\n",
    "    \n",
    "    filtered_spark = filtered_spark.drop(*['recording-MBID'])\n",
    "    filtered_data = filtered_spark.toPandas()\n",
    "    \n",
    "    return(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int_ID(df, user_buffer, track_buffer):\n",
    "    \"\"\"\n",
    "    Converts the dataframe's string based ID;s for users and recordings.\n",
    "    \n",
    "    Input\n",
    "        df; Dataframe, the dataset implemented in the simulation\n",
    "        user_buffer; int, buffer added since data is loaded in batches\n",
    "        track_buffer; int, buffer added since data is loaded in batches\n",
    "    Output\n",
    "        df; Dataframe, the updated dataframe\n",
    "        user_buffer; int, the updated buffer\n",
    "        track_buffer; int, the updated buffer\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.rename(columns = {'recording-MBID':'recordingMBID', 'uuid':'userMBID'})\n",
    "    \n",
    "    # Create a dictionary of ints ranging from 0 to length of unique uuids\n",
    "    item_lookup = pd.Series(dict(enumerate(df.recordingMBID.unique())), name = 'MBID')\n",
    "    user_lookup = pd.Series(dict(enumerate(df.userMBID.unique())))\n",
    "    \n",
    "    # Swap the index and value of the lookups.\n",
    "    user_lookup_2 = (pd.Series(user_lookup.index.values, index=user_lookup ))\n",
    "    item_lookup_2 = (pd.Series(item_lookup.index.values, index=item_lookup)) \n",
    "    \n",
    "    # Map the values to dataset.\n",
    "    df['userID'] = df['userMBID'].map(user_lookup_2)\n",
    "    df['trackID'] = df['recordingMBID'].map(item_lookup_2)\n",
    "    \n",
    "    # Increment each ID by the buffers\n",
    "    df['userID'] = df[\"userID\"] + user_buffer\n",
    "    df['trackID'] = df['trackID'] + track_buffer\n",
    "\n",
    "    # Update buffers\n",
    "    user_buffer = max(df['userID'])\n",
    "    track_buffer = max(df['trackID'])\n",
    "    return(df, user_buffer, track_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_data(users):\n",
    "    \"\"\"\n",
    "    Returns a list of users that are will the required values as decided by from EDA\n",
    "    \n",
    "    Input\n",
    "        users; Dataframe of users before pruning\n",
    "    Output\n",
    "        users; Dataframe of users after preprocessing\n",
    "    \"\"\"\n",
    "    users = users[users['mainstreamness_track'].between(0, 0.28)]\n",
    "    users = users[users['exploratoryness_track'].between(0.96, 1)]\n",
    "    users = users[users['age'].between(18, 35)]\n",
    "    \n",
    "    return(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitelists_users():\n",
    "    \"\"\"\n",
    "    Obtains the list of whiteliest users from the mbdump.\n",
    "    \n",
    "    Returns:\n",
    "        whitelisted_users; Pandas series, the datastored on every whitelisted user.\n",
    "    \"\"\"\n",
    "    # Loads the two datasets containing users\n",
    "    user_demographics = pd.read_csv('../dirty_data/MLHD_demographics.csv', sep = '\\t')\n",
    "    user_demographics = user_demographics[['uuid','age', 'country', 'playcount']]\n",
    "    user_behaviour = pd.read_csv('../dirty_data/MLHD_behavioural_features.csv', sep = '\\t')\n",
    "    user_behaviour = user_behaviour[['uuid','exploratoryness_artist', 'exploratoryness_track', 'mainstreamness_artist','mainstreamness_track']]\n",
    "    \n",
    "    # Merges files together\n",
    "    users = pd.merge(user_demographics, user_behaviour, on = \"uuid\")\n",
    "    \n",
    "    # Prunes list of data\n",
    "    whitelisted_users = strip_data(users)['uuid']\n",
    "    whitelisted_users.to_pickle('../cleaned_data/users.pkl')\n",
    "    return(whitelisted_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_by_country(country):\n",
    "    \"\"\"\n",
    "    Loads data for every whitelisted user based on their country in batches.\n",
    "    Input:\n",
    "        country; string, the users country abbreviation.\n",
    "    Output;\n",
    "        data; dataframe, the resulting dataframe of user histories.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        users = pd.read_pickle('../cleaned_data/users.pkl')\n",
    "    except:\n",
    "        print(\"users.pkl file not found loading from mbdump\")\n",
    "        users = whitelist_users()\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        artists = pd.read_csv('../cleaned_data/artists.csv', index_col = [0])\n",
    "    except: \n",
    "        print(\"artists.pkl not found, loading artists\")\n",
    "        artists = load_artists()\n",
    "        \n",
    "    # Filters users by their country   \n",
    "    country_users = users[users['country'] == country.upper()]\n",
    "    \n",
    "    file_number = 0\n",
    "    finished = False\n",
    "    \n",
    "    batches = []\n",
    "    user_buffer = 0\n",
    "    track_buffer = 0\n",
    "    while finished == False:\n",
    "        \n",
    "        # Gets the number of times each user has listened each track\n",
    "        test_data, finished, file_number = get_value_count(country_users, file_number, 'recording-MBID')\n",
    "\n",
    "        # Adds the tracks names to the dataset\n",
    "        data  = add_track_names(test_data)\n",
    "\n",
    "        # Adds the interger based ID for users and tracks.\n",
    "        data, user_buffer, track_buffer = convert_to_int_ID(data, user_buffer, track_buffer)\n",
    "\n",
    "        # Makes sure the count data is an integer\n",
    "        data['count'] = data['count'].astype('int64')\n",
    "        \n",
    "        # Merges the data with artist id and name\n",
    "        data =  pd.merge(data, artists[['artist-MBID','artistName']], on=\"artist-MBID\", how=\"left\")\n",
    "   \n",
    "        batches.append(data)\n",
    "        print(\"batch: \", len(batches))\n",
    "        \n",
    "    data = pd.concat(batches)\n",
    "    # Removes any nans.\n",
    "    data = data.mask(data.eq('None')).dropna()\n",
    "    return(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
